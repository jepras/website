---
title: "Project: Action on Sent"
date: "2025-12-07"
description: "Writeup of 'Action on Sent' - a project automating actions every time an email is sent"
tags: ["ai-agents", "python", "automations"]
category: "Project"
---

<BlogIntro
  title="Project: Action on Sent"
  description="Writeup of 'Action on Sent' - a project automating actions every time an email is sent"
  readTime="5 min"
  author="Jeppe Rasmussen"
  tags={["ai-agents", "python", "automations"]}
  date="2025-12-07"
  variant="featured"
/>

*Press send. Do the associated task. Send another email. Do the associated task.
.. And often the task is defined directly in the sent email* ü§¶‚Äç‚ôÇÔ∏è

One busy director, in need of more hours in the day, was tired by this loop. They wanted to save the time and the mental energy drained by doing follow up tasks after emails were sent. It felt inefficient to do this menial grunt work again and again. 

With the unstructured & varying data in emails, then this couldn't be done with regular automations. So we decided to create an AI Agent system that could relieve him of one of these mundane tasks, and then expand to more workflows later.

This is a writeup of "Action on Sent".

<HoverCard 
  trigger="> Hover for background about me if you think that is important" 
  triggerClassName="text-muted text-sm"

>
  <div className="space-y-2">
    <h4 className="font-semibold text-foreground !mt-0">Jeppe Rasmussen</h4>
    <p className="text-sm text-muted-foreground">
      Technology passionate, but not an engineer by education. I worked previously at Microsoft as a project manager I started learning to program the 2017 way 8 years ago, but then got away from programming the last 6 years as i founded the IoT company, Tector. I made the first proof of concept until we got a CTO. I was always heavily involved in product development, but not actively developing the product. 
    </p>
    <p className="text-sm text-muted-foreground">
        Now I‚Äôm back trying to refine my technical skills by learning how to create AI systems and reflecting a ton on how one needs to learn to program in 2025.
    </p>
    
  </div>
</HoverCard> 
<br />
<HoverCard 
  trigger="> Hover if you are a senior dev" 
  triggerClassName="text-muted-foreground text-sm"

>
  <div className="space-y-2">
    <h4 className="font-semibold text-foreground !mt-0">Thank you for reading</h4>
    <p className="text-sm text-muted-foreground">
      If you are a senior developer, then you‚Äôll probably laugh at many of my decisions and learnings. Feel free to do that, or instead you can read this piece to understand what goes on inside the mind of a rookie AI assisted developer. 
    </p>
    <p className="text-sm text-muted-foreground">
     I strongly believe that I am learning so much faster than when I learned to program 8 years ago. I am not spending months getting a basic frontend to-do list app made that no one will use. I‚Äôm actually creating systems that people are using and learning quickly when taking the wrong decisions. 
    </p>
    
  </div>
</HoverCard> 

<br /><br />
<Separator />

## Description
The system serves as a bridge between email communications and CRM management, automating the process of capturing sales opportunities in the CRM. 

It addresses the common business challenge of ensuring that potential sales leads from email communications are properly tracked in a CRM system without requiring manual data entry.

It works in the following way:

1. The AI Orchestrator is pinged when an email webhook triggers.
2. The Analysis AI analyses the text and decides whether it‚Äôs a sales deal.
3. The CRM AI looks for duplicates in the CRM and creates necessary deal and objects.
4. The Logging AI logs the outcome of the other AI‚Äôs for transparency & future improvements.

<AnimatedSystemDiagram />
### Why the use of an AI Agent flow for this instead of Zapier/Make?

The email analyser makes text analysis hassle free. Deciding whether something is a potential deal = easy.

```
Show example of emails that are deals versus those that aren‚Äôt. 
```

The AI Agent can extract context from the entire email chain and fill out fields as needed and fill in empty values when none is found.

Having worked with CRM automation before, it is a hassle to work with missing fields, missing context from long conversations, etc. An automation that creates objects wrongfully and has to be changed destroys the whole purpose of it.

An AI can do this job much better.

<EmailExtractionDiagram />

The AI analyses the sent email. Understands that it is an offer. Also extracts Bob‚Äôs full name and organisation from the email as it decides his email address doesn‚Äôt give the right info. It also extracts a short summary from the full email chain. And it extracts that the next step is for Hans to get back.

This can easily be extracted to some structured output and is then easy to use to check for duplicates, both strict and fuzzy logic. 

The CRM agent then sees the contact email doesn‚Äôt exists, so it creates it. The organisation does exist though, so it connects the new contact with the organisation. The contact doesn‚Äôt have any pre existing deals, so a new one is created with the right name, organisation, an email chain summary in the notes and follow up task for Bob to follow up in 2 days as Hans wants to get this done before next week.

<EmailToCrmPipeline />

This system could have been done with just 1 agent, but giving each agent different context increases the likelihood of correct outcomes. It might have been overkill for this project, but we wanted to make the project future proof and ready to automate more automations from emails in the future.

<Separator />


The rest of this write up is for different audiences.. so click below if you: 
- want to read more about the technical considerations
- are curious about my learnings (mostly related to how to work with LLMs)
- think that there must be some bigger potential with this application?

<Separator />

## Tech stack
*You can skim an accurate & extensive, though not that interesting nor filtered, description of all the technical aspects of the project in this [deepwiki here](https://deepwiki.com/jepras/supa-vercel-infra).*

<TechStackBento 
  items={[
    { icon: 'nextjs', iconType: 'devicon', colSpan: 1, rowSpan: 1 },
    { icon: 'typescript', iconType: 'devicon', colSpan: 1, rowSpan: 1 },
    { icon: 'https://avatars.githubusercontent.com/u/139895814?v=4', iconType: 'url', colSpan: 1, rowSpan: 1 },
    { icon: 'tailwindcss', iconType: 'devicon', colSpan: 1, rowSpan: 1 },
    { icon: 'python', iconType: 'devicon', colSpan: 1, rowSpan: 1 },
    { icon: 'https://fastapi.tiangolo.com/img/logo-margin/logo-teal.png', iconType: 'url', colSpan: 1, rowSpan: 1 },
    { icon: 'https://companieslogo.com/img/orig/supabase_BIG.D-94f7cfaf.png?t=1720244494', iconType: 'url', colSpan: 1, rowSpan: 1 },
    { icon: 'https://registry.npmmirror.com/@lobehub/icons-static-png/latest/files/dark/openrouter-text.png', iconType: 'url', colSpan: 1, rowSpan: 1 },
    { icon: 'https://registry.npmmirror.com/@lobehub/icons-static-png/latest/files/dark/vercel-text.png', iconType: 'url', colSpan: 1, rowSpan: 1 },
    { icon: 'https://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/Docker_%28container_engine%29_logo.svg/2560px-Docker_%28container_engine%29_logo.svg.png', iconType: 'url', colSpan: 2, rowSpan: 1 },
    { icon: 'https://railway.com/brand/logotype-light.png', iconType: 'url', colSpan: 1, rowSpan: 1 },
    { icon: 'https://lever-client-logos.s3.us-west-2.amazonaws.com/8d1de9da-4d2c-4e16-9a16-afd5dafb36d7-1667892402176.png', iconType: 'url', colSpan: 2, rowSpan: 1 },
    { icon: 'https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Outlook.com_logo_and_wordmark_%282012-2019%29.svg/2560px-Outlook.com_logo_and_wordmark_%282012-2019%29.svg.png', iconType: 'url', colSpan: 2, rowSpan: 1 },
  ]} 
  columns={4} 
/>

My own aim with this project was to learn how to build a production ready AI Agent system using LLMs. There are a few steps in that:

1. Figuring out the system architecture
2. Figuring out what deployment providers to use
3. Decide where on the scale of locally ready to enterprise ready the system should be
4. Figuring out what LLM to use

Working with LLMs best practices state that one should discuss with multiple LLMs before generating a final specification doc that then can be used with an execution agent like Claude Code, Gemini CLI or Cursor.

When doing that it‚Äôs important to remind the LLM to consider the scale of your application and what your learning goals actually are. It will often suggest very scaleable systems, which might be overkill for the beginning of a project.

- **Frontend**: Next.js, TypeScript, Tailwind CSS, Shadcn/ui components (deployed on Vercel)
- **Backend**: Python FastAPI application (deployed on Railway)
- **Database & Auth**: Supabase
- **Integrations**: Pipedrive CRM, Microsoft Outlook
- **AI**: LLM API for sales opportunity detection. Use OpenRouter
- **Deployment**: Vercel (frontend) + Railway (backend) through Dockerfiles
- **Triggers**: Webhook-driven when user sends emails
- **Real-time**: Supabase subscriptions for live dashboard updates

<MermaidDiagram 
  title="System Architecture"
  variant="subtle"
  chart={`
graph LR
    subgraph "Frontend"
        A[Next.js App]
        B[Vercel]
    end
    
    subgraph "Backend"
        C[FastAPI]
        D[Railway]
    end
    
    subgraph "Database"
        E[Supabase]
    end
    
    subgraph "AI Services"
        F[OpenRouter LLM]
    end
    
    subgraph "Integrations"
        G[Outlook Webhook]
        H[Pipedrive CRM]
        I[Outlook sent emails]
    end
    
    A --> C
    C --> E
    C --> F
    G --> C
    C --> H
    C --> I
    
    style A fill:#10b981,stroke:#10b981,color:#fff
    style C fill:#3b82f6,stroke:#3b82f6,color:#fff
    style E fill:#8b5cf6,stroke:#8b5cf6,color:#fff
    style F fill:#f59e0b,stroke:#f59e0b,color:#fff
  `}
/>

<MermaidDiagram 
  title="Data Flow Sequence"
  variant="bordered"
  chart={`
sequenceDiagram
    participant U as User
    participant E as Email Client
    participant W as Webhook
    participant AI as AI Orchestrator
    participant CRM as Pipedrive
    
    U->>E: Send Email
    E->>W: Trigger Webhook
    W->>AI: Process Email
    AI->>AI: Analyze Content
    AI->>CRM: Check for Duplicates
    CRM-->>AI: Return Results
    AI->>CRM: Create/Update Objects
    CRM-->>AI: Confirm Changes
    AI-->>W: Log Outcome
  `}
/>

<MermaidDiagram 
  title="Step by step flow diagram"
  variant="subtle"
  enableZoom={true}
  chart={`
graph LR
    subgraph "Email Processing Pipeline"
        A[Email Webhook] --> B[Webhook Validation]
        B --> C[Email Extraction]
        C --> D[Content Analysis]
    end

    subgraph "AI Analysis Layer"
        D --> E[OpenRouter Client]
        E --> F[Model Selection]
        F --> G[Prompt Engineering]
        G --> H[AI Response]
        H --> I[Result Parsing]
    end

    subgraph "Business Logic"
        I --> J{Opportunity?}
        J -->|Yes| K[Deal Check]
        J -->|No| L[Log Result Only]
        K --> M{Deal Exists?}
        M -->|No| N[Create Deal]
        M -->|Yes| O[Log Existing Deal]
    end

    subgraph "Data Storage"
        L --> P[Activity Logs]
        N --> P
        O --> P
        P --> Q[Real-time Updates]
    end

    subgraph "User Interface"
        Q --> R[Dashboard]
        R --> S[Email Viewer]
        R --> T[AI Analysis Display]
        R --> U[Deal Status]
        R --> V[Cost Monitor]
    end

    style E fill:#ff9999
    style F fill:#ff9999
    style G fill:#ff9999
    style H fill:#ff9999
    style V fill:#99ccff
  `}
/>

So as you can read, an attempt to not overcomplicate things before it is necessary - but also have it production ready for a client. 

<MarkdownAccordion title="AI assisted devs make flawed applications" defaultOpen={false}>
If I had to learn programming all from scratch and all security best practices from scratch, then it would take me +5 years to make applications that are not flawed. 

I am a rookie developer, so I don‚Äôt know all my blind spots. When implementing an enterprise ready solution I‚Äôd always consult with a deployment & security consultant. And an experienced programmer to ensure reliability in my code. 

But to get started on projects I believe LLMs can make me aware of at least 80% of potential issues. 

In this project i have implemented these best practices for reliability, deployment & security:

- Containerized deployment
- Comprehensive monitoring
- Token security
- Row level security
- Error handling & retry logic (Devin tells me this, but I know that this is not done throughout the entire codebase as I have prioritised speed to get to a working MVP)
- Environment configuration

Devin has made me aware of these following reliability, deployment & security best practices I could work on:

- Distributed tracing
- Proactive alerting
- Circuit breakers
- Backup & recovery
- Security headers

I also know myself that I need to improve on CORS. I also know I haven‚Äôt double checked the types. 

So at least I know some of the areas to improve. If I didn‚Äôt have LLMs to help me, then it would take a long time to understand what I was missing in this project. 

Furthermore, when working with LLMs, like working with any tool, the user needs to be aware of it‚Äôs constraints & weaknesses. That is why I do read on how to prevent prompt injecting & where to be aware in the development flow (like making sure the LLM doesn‚Äôt get your env variables).
</MarkdownAccordion>
<MarkdownAccordion title="My work on the logging aspect is not done yet, but you can read draft thoughts here" defaultOpen={false}>
One of the main things i wanted to learn was to make production ready AI automations instead of basic n8n automations where i wasn‚Äôt in control and would be worried about it crashing. This was a simple use case, but i wanted to treat it as a critical workflow as i wanted to give the client assurance that more things could be automated. 

Cursor did an okay job in creating some basic performance monitoring and application logic monitoring. However, it clearly occured to me how huge a topic this is and that the v2 of this project will definitely be about how to implement proper best practice monitoring with OpenTelemetry, Sentry & LangSmith. 

The current version has some monitoring in it and shown in the frontend, but this will be separated into a different database & admin dashboard with only the backend only fetching specific business events and showing them in the frontend. 

Given the latest news about exploding unintended LLM costs, I started implementing limits & trackers on the API calls & AI operations. The second objective was to log business events properly, as that was key for me to understand when the solution starts getting used.

![Alt text for accessibility](/blog/project-action-on-sent/system-logs.png)

</MarkdownAccordion>